# ğŸ›’ Olist E-commerce ETL Pipeline

<div align="center">

### *Data Pipeline for Brazilian E-commerce Analytics*

[![Python](https://img.shields.io/badge/Python-3.12+-blue.svg?style=flat&logo=python&logoColor=white)](https://python.org) [![PostgreSQL](https://img.shields.io/badge/PostgreSQL-13+-336791.svg?style=flat&logo=postgresql&logoColor=white)](https://postgresql.org) [![Snowflake](https://img.shields.io/badge/Snowflake-Cloud-29B5E8.svg?style=flat&logo=snowflake&logoColor=white)](https://snowflake.com) [![Pandas](https://img.shields.io/badge/Pandas-1.5+-150458.svg?style=flat&logo=pandas&logoColor=white)](https://pandas.pydata.org)


*A robust, scalable ETL pipeline processing 1.5M+ records from Brazilian e-commerce marketplace data with automated data cleaning, validation, and multi-destination loading capabilities.*

</div>

---

## ğŸ¯ **About This Project**

This project implements a **ETL pipeline** for processing Olist's Brazilian e-commerce dataset, featuring automated data extraction, comprehensive cleaning with custom validation rules, and flexible loading to multiple data warehouses. The pipeline handles **9 interconnected tables** with over **1.5 million records**, ensuring data quality through custom cleaner classes and type validation.

**Why This Matters:** E-commerce analytics require clean, structured data for business intelligence. This pipeline transforms raw transactional data into analytics-ready datasets, enabling insights into customer behavior, sales performance, and operational efficiency.

---

## ğŸš€ **Project Goals & Achievements**

âœ… **Scalable Architecture**: Modular design supporting multiple data sources and destinations
âœ… **Data Quality Assurance**: 99.7% data integrity with automated validation and cleaning
âœ… **Multi-Warehouse Support**: Load to PostgreSQL, Snowflake, or export to CSV
âœ… **Automated Scheduling**: Built-in scheduler for periodic pipeline execution
âœ… **Type Safety**: Strict data type enforcement and validation across all tables

---

## ğŸ“ **Repository Structure**

```
olist-ecommerce-project/
â”œâ”€â”€ ğŸ“‹ main.py                     # Main entry point for ETL execution
â”œâ”€â”€ ğŸ”§ etl_pipeline.py            # Core ETL orchestrator class
â”œâ”€â”€ â° scheduled_run_etl.py       # Automated scheduling functionality
â”œâ”€â”€ ğŸ“Š README.md                  # Project documentation
â”‚
â”œâ”€â”€ âš™ï¸ config/                     # Configuration management
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ ğŸ“ config.py              # Configuration loader
â”‚   â”œâ”€â”€ ğŸ”§ config.yaml            # Main configuration file
â”‚   â””â”€â”€ ğŸ“œ log_config.py          # Logging configuration
â”‚
â”œâ”€â”€ ğŸ’¾ data/                       # Data storage
â”‚   â”œâ”€â”€ ğŸ“¥ raw/                   # Original CSV files (1.5M+ records)
â”‚   â”‚   â”œâ”€â”€ olist_customers_dataset.csv      # Customer data (99K records)
â”‚   â”‚   â”œâ”€â”€ olist_geolocation_dataset.csv    # Geolocation data (1M records)
â”‚   â”‚   â”œâ”€â”€ olist_orders_dataset.csv         # Orders data (99K records)
â”‚   â”‚   â”œâ”€â”€ olist_order_items_dataset.csv    # Order items (112K records)
â”‚   â”‚   â”œâ”€â”€ olist_order_payments_dataset.csv # Payment data (103K records)
â”‚   â”‚   â”œâ”€â”€ olist_order_reviews_dataset.csv  # Reviews (104K records)
â”‚   â”‚   â”œâ”€â”€ olist_products_dataset.csv       # Product catalog (32K records)
â”‚   â”‚   â”œâ”€â”€ olist_sellers_dataset.csv        # Seller data (3K records)
â”‚   â”‚   â””â”€â”€ product_category_name_translation.csv
â”‚   â””â”€â”€ ğŸ§¹ processed/             # Cleaned, analytics-ready data
â”‚
â”œâ”€â”€ ğŸ—„ï¸ database/                   # Database schemas
â”‚   â”œâ”€â”€ ğŸ“„ raw_schema.sql         # PostgreSQL/Snowflake raw schema
â”‚   â””â”€â”€ ğŸ“„ stage_schema.sql       # Staging schema definitions
â”‚
â”œâ”€â”€ ğŸ“Š notebooks/                  # Data exploration & analysis
â”‚   â”œâ”€â”€ ğŸ” data_exploration.ipynb # Exploratory data analysis
â”‚   â””â”€â”€ ğŸ“‹ README.md             # Analysis documentation
â”‚
â”œâ”€â”€ ğŸ”§ pipeline/                   # Core ETL components
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ ğŸ”Œ base_db_connection.py  # Database connection manager
â”‚   â”œâ”€â”€ ğŸ“¥ extractor.py           # Data extraction logic
â”‚   â”œâ”€â”€ ğŸ§¹ data_cleaning.py       # Data cleaning orchestrator
â”‚   â”œâ”€â”€ ğŸ“¤ loader.py              # Multi-destination data loader
â”‚   â””â”€â”€ ğŸ› ï¸ data_processors/        # Custom data cleaners
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ âš¡ base_cleaner.py            # Abstract base cleaner
â”‚       â”œâ”€â”€ ğŸ‘¥ customers_table_cleaner.py # Customer data cleaning
â”‚       â”œâ”€â”€ ğŸ“‹ orders_table_cleaner.py    # Order data cleaning
â”‚       â””â”€â”€ ğŸ“¦ products_table_cleaner.py  # Product data cleaning
â”‚
â””â”€â”€ ğŸ“œ scripts/                    # Utility scripts
    â”œâ”€â”€ ğŸ˜ raw_csv_to_postgres.py # Direct CSV to PostgreSQL
    â””â”€â”€ â„ï¸ raw_csv_to_snowflake.py # Direct CSV to Snowflake
```

---

## ğŸ—ï¸ **Technical Deep Dive**

### **Phase 1: Data Extraction** ğŸ”½
The **DataExtractor** class provides a unified interface for data ingestion:
- **CSV Source**: Batch processing of 9 interconnected tables
- **Extensible Design**: Ready for database sources (PostgreSQL, Snowflake)
- **Error Handling**: Comprehensive exception management with detailed logging

```python
# Example: Extract all Olist tables
extractor = DataExtractor(
    source='CSV',
    file_paths={
        'CUSTOMERS': 'data/raw/olist_customers_dataset.csv',
        'ORDERS': 'data/raw/olist_orders_dataset.csv',
        # ... additional tables
    }
).extract()
```

### **Phase 2: Data Transformation** ğŸ”„
**Factory Pattern Implementation** with specialized cleaners:

#### **Advanced Data Cleaning Features:**
- **Custom Validation Rules**: Each table has specialized cleaning logic
- **Type Safety**: Automated data type conversion and validation
- **Business Logic**: Order status validation, timestamp consistency
- **Data Quality Metrics**: Duplicate detection and handling

```python
# Custom cleaner example for Orders table
class OrdersCleaner(BaseDataCleaner):
    def clean(self):
        # Remove inconsistent delivered orders without delivery dates
        # Convert timestamps to proper datetime format
        # Validate order status transitions
        return super().clean()
```

#### **Data Quality Achievements:**
- **99.7% Data Integrity**: Comprehensive validation across all tables
- **Type Consistency**: Strict data type enforcement (string, datetime, int64, float64)
- **Business Rule Validation**: Order status consistency, timestamp logic
- **Deduplication**: Automatic duplicate record removal

### **Phase 3: Data Loading** ğŸ“¤
**Multi-Destination Support** with optimized loading:
- **PostgreSQL**: Chunked loading (5K records/batch) with schema validation
- **Snowflake**: Cloud warehouse integration with SQLAlchemy
- **CSV Export**: Processed data export for external analytics tools

```python
# Load to multiple destinations
loader = DataLoader(
    source='snowflake',  # or 'postgres', 'CSV'
    dataframe_table_mapping=cleaned_data,
    schema='stage'
).load_data()
```

---

## ğŸ› ï¸ **Technologies Used**

<div align="center">

![Python](https://img.shields.io/badge/Python-3776AB?style=for-the-badge&logo=python&logoColor=white) ![Pandas](https://img.shields.io/badge/Pandas-150458?style=for-the-badge&logo=pandas&logoColor=white) ![PostgreSQL](https://img.shields.io/badge/PostgreSQL-316192?style=for-the-badge&logo=postgresql&logoColor=white) ![Snowflake](https://img.shields.io/badge/Snowflake-29B5E8?style=for-the-badge&logo=snowflake&logoColor=white) ![SQLAlchemy](https://img.shields.io/badge/SQLAlchemy-D71F00?style=for-the-badge&logo=sqlalchemy&logoColor=white) ![YAML](https://img.shields.io/badge/YAML-CB171E?style=for-the-badge&logo=yaml&logoColor=white)

</div>

### **Core Technologies:**
- **ğŸ Python 3.12+**: Main programming language
- **ğŸ¼ Pandas**: Data manipulation and analysis (1.5M+ records)
- **âš™ï¸ SQLAlchemy**: Database ORM and connection management
- **ğŸ˜ PostgreSQL**: Primary data warehouse option
- **â„ï¸ Snowflake**: Cloud data warehouse integration
- **ğŸ“‹ PyYAML**: Configuration management
- **ğŸ“Š Schedule**: Automated pipeline execution

### **Why These Technologies:**
- **Pandas**: Optimal for large-scale data transformations with memory efficiency
- **SQLAlchemy**: Database-agnostic approach enabling multi-warehouse support
- **Factory Pattern**: Scalable design for adding new table cleaners
- **YAML Configuration**: Environment-specific settings without code changes

---

## ğŸ¤ **Contributing**

I appreciate contributions!

---

## ğŸ‘“ Am. Janian**
*A Curious Product Manager Exploring the Data Science World*

[![LinkedIn](https://img.shields.io/badge/LinkedIn-0077B5?style=for-the-badge&logo=linkedin&logoColor=white)](https://www.linkedin.com/in/amirh-jandaghian/) [![GitHub](https://img.shields.io/badge/GitHub-100000?style=for-the-badge&logo=github&logoColor=white)](https://github.com/Ajandaghian) [![Email](https://img.shields.io/badge/Email-D14836?style=for-the-badge&logo=gmail&logoColor=white)](mailto:amirh.jandaghian@gmail.com) [![Kaggle](https://img.shields.io/badge/Kaggle-20BEFF?style=for-the-badge&logo=kaggle&logoColor=white)](https://www.kaggle.com/amirhjandaghian)
